{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Laboratory\n",
    "Institute of Imaging and Computer Vision, RWTH Aachen\n",
    "\n",
    "Version SS2024\n",
    "\n",
    "### Session 3: Neural Networks & Backpropagation\n",
    "\n",
    "\n",
    "### Goal of this Session\n",
    "\n",
    "In this session you will, step by step, implement a the backpropagation algorithm yourself without using any deep learning libraries. You should already be familiar with Python as well as NumPy (a package for scientific computing with Python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 0:** Please execute the following cell which is a workaround for data loading and ignore it until further notice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import urllib\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class MNIST:\n",
    "    def __init__(self):\n",
    "        # Alternative method to load MNIST, if mldata.org is down\n",
    "        from scipy.io import loadmat\n",
    "        dset_path = os.path.join(os.path.abspath(os.environ[\"HOME\"]), 'datasets')\n",
    "        mnist_path = os.path.join(dset_path, \"mnist-original.mat\")\n",
    "        try:\n",
    "            mnist_raw = loadmat(mnist_path)\n",
    "        except FileNotFoundError:\n",
    "            if not os.path.isdir(dset_path):\n",
    "                os.makedirs(dset_path)\n",
    "            mnist_alternative_url = \"https://github.com/amplab/datascience-sp14/raw/master/lab7/mldata/mnist-original.mat\"\n",
    "            response = urllib.request.urlopen(mnist_alternative_url)\n",
    "            with open(mnist_path, \"wb\") as f:\n",
    "                content = response.read()\n",
    "                f.write(content)\n",
    "            mnist_raw = loadmat(mnist_path)\n",
    "\n",
    "        self.data = mnist_raw[\"data\"].T\n",
    "\n",
    "        onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "        labels = mnist_raw[\"label\"][0].reshape(len(mnist_raw[\"label\"][0]), 1)\n",
    "        labels = onehot_encoder.fit_transform(labels)\n",
    "        \n",
    "        self.target = labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Task:** Familiarize yourself briefly with the given code. Pay particural attention to the `Layer` and `Cost` classes, from which you will derive the classes you implement, and the `Sigmoid` layer which is predefined as an example. You'll also need to execute the cells in this section once.\n",
    "\n",
    "Since we want to design our network with 10 output neurons and train it in a way that the output activation for the digit j becomes 1.0 at the j-th neuron, we need to vectorize our label.\n",
    "\n",
    "**Task 1a:** Implement the function `vectorize()` which accepts an integer $j$ between 0 and 9 and returns a 10-dimensional unit vector with a 1.0 in the $j$-th position and zeroes elsewhere.  This is used to convert a digit (0...9) into a corresponding desired output from the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def vectorize(j):\n",
    "    # Your code here.\n",
    "\n",
    "    return label_vector\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    mnist = MNIST()\n",
    "    images, labels = mnist.data, mnist.target\n",
    "\n",
    "    image_size = images.shape[1]\n",
    "    label_size = labels.shape[1]\n",
    "\n",
    "    random_permutation = np.random.permutation(images.shape[0])\n",
    "    images = images[random_permutation, :]\n",
    "    labels = labels[random_permutation, :]\n",
    "    \n",
    "    images = (images - np.mean(images))/np.std(images)\n",
    "\n",
    "    return images, labels, image_size, label_size\n",
    "\n",
    "\n",
    "def train(net, cost_function, number_epochs, batch_size, learning_rate):\n",
    "    images, labels, image_size, label_size = load_data()\n",
    "    training_images, validation_images = images[:50000], images[50000:]\n",
    "    training_labels, validation_labels = labels[:50000], labels[50000:]\n",
    "\n",
    "    for e in range(number_epochs):\n",
    "        cost = train_epoch(e, net, training_images, training_labels, cost_function, batch_size, learning_rate)\n",
    "        accuracy = validate_epoch(e, net, validation_images, validation_labels, batch_size)\n",
    "        print('cost=%5.6f, accuracy=%2.6f' % (cost, accuracy), flush=True)\n",
    "\n",
    "\n",
    "def train_epoch(e, net, images, labels, cost_function, batch_size, learning_rate):\n",
    "    epoch_cost = 0\n",
    "\n",
    "    for i in tqdm(range(0, len(images), batch_size), ascii=False, desc='training,   e=%i' % e):\n",
    "        batch_images = images[i:min(i + batch_size, len(images)), :]\n",
    "        batch_labels = labels[i:min(i + batch_size, len(labels)), :]\n",
    "\n",
    "        # zero the gradients\n",
    "        net.zero_gradients()\n",
    "\n",
    "        # forward pass\n",
    "        prediction = net.forward(batch_images)\n",
    "        cost = cost_function.estimate(batch_labels, prediction)\n",
    "\n",
    "        # backward pass\n",
    "        dprediction = cost_function.gradient(cost)\n",
    "        net.backward(dprediction)\n",
    "\n",
    "        # update the parameters using the computed gradients via stochastic gradient descent.\n",
    "        net.update_parameters(learning_rate)\n",
    "\n",
    "        epoch_cost += np.mean(cost)\n",
    "\n",
    "    return epoch_cost\n",
    "\n",
    "\n",
    "def validate_epoch(e, net, images, labels, batch_size):\n",
    "    n_correct = 0\n",
    "    n_total = 0\n",
    "\n",
    "    for i in tqdm(range(0, len(images), batch_size), ascii=False, desc='validation, e=%i' % e):\n",
    "        batch_images = images[i:min(i + batch_size, len(images)), :]\n",
    "        batch_labels = labels[i:min(i + batch_size, len(labels)), :]\n",
    "\n",
    "        # compute predicted probabilities.\n",
    "        predictions = net.forward(batch_images)\n",
    "\n",
    "        # find the most probable class label.\n",
    "        n_correct += sum(np.argmax(batch_labels, axis=1) == np.argmax(predictions, axis=1))\n",
    "        n_total += batch_labels.shape[0]\n",
    "\n",
    "    return n_correct / n_total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1a:** Implement the sigmoid function! Make sure it can handle a vector of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_function(var):\n",
    "    # Your code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1b:** For later use in the backpropagation algorithm calculate the first derivative of the sigmoid function and implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(z):\n",
    "    # Your code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-4, 4, num=100)\n",
    "plt.plot(x, sigmoid_function(x))\n",
    "plt.plot(x, sigmoid_derivative(x))\n",
    "plt.legend((\"sigmoid\", \"derivative\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following abstract classes should serve as parent classes for all the different layers and cost functions which you will implement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        # Initialize all member variables of the layer.\n",
    "        pass\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        # Implemets for forward pass of the layer and returns x_out.\n",
    "        pass\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        # Implements the backward pass of the layer and returns d_in.\n",
    "        pass\n",
    "\n",
    "    def zero_gradients(self):\n",
    "        # Sets all gradients of the layer to zero.\n",
    "        pass\n",
    "\n",
    "    def update_parameters(self, learning_rate):\n",
    "        # Update the parameters of the layer with the help of the gradients stored during the backward pass.\n",
    "        pass\n",
    "\n",
    "\n",
    "class Cost:\n",
    "    def __init__(self):\n",
    "        # Initialize all member variables of the cost function.\n",
    "        pass\n",
    "\n",
    "    def estimate(self, target, prediction):\n",
    "        # Estimates and return the cost with respect to the predicted label and a target label previously set by set_target().\n",
    "        pass\n",
    "\n",
    "    def gradient(self, cost):\n",
    "        # Calculates and returns the gradient with respect to the cost.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class derived from the Layer class implements the forward and backward pass of the sigmoid activation function alread known from the previous session and serves as an example for you. Since it does not have learnable parameters, no `update_parameters` or `zero_gradients` function needs to be implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        self.x_in = None\n",
    "    \n",
    "    def forward(self, x_in):\n",
    "        self.x_in = x_in\n",
    "        x_out = sigmoid_function(x_in)\n",
    "        return x_out\n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        d_in = d_out * sigmoid_derivative(self.x_in)\n",
    "        return d_in\n",
    "    \n",
    "    def zero_gradients(self):\n",
    "        pass\n",
    "    \n",
    "    def update_parameters(self, learning_rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Implementation\n",
    "\n",
    "**Task 2a:** Implement the `forward` function for the `Linear` layer. Remember to store the `x_in` for use in the backward pass.\n",
    "\n",
    "**Task 2b:** Implement the `estimate` function for the `MeanSquareError` cost, which estimates the cost after a ground truth target is set. Remember to store the `prediction` for calculating the gradient.\n",
    "\n",
    "**Task 2c:** Implement the `gradient` function for the `MeanSquareError` cost, which calculates the gradient with respect to the cost. Use the `prediction` stored during the forward pass.\n",
    "\n",
    "**Task 2d:** Implement the `backward` function for the `Linear` layer. The function should also calculate and accumulate the gradient of `w` and `b` with regard to the error.\n",
    "\n",
    "**Task 2e:** Implement the `update_parameters` function for the `Linear` layer, i.e., use the gradients `dw` and `db` together with a given `learning_rate` to update the parameters `w` and `b` accordingly.\n",
    "\n",
    "**Task 2f:** Test your implementation by propagating random input through a linear layer followed by a sigmoid layer, estimating the mean square error to a random target, calculating the gradient and propagating it back through the sigmoid and linear layer. Afterwards update the parameters of the linear layer using the function you implemented.\n",
    "\n",
    "**Task 3a:** Implement the `Network` class which can encapsulate multiple layers. It offers the same interface as a layer and is therefore derived from the `Layer` parent as well. Make sure to implement all member functions needed. The `forward` function propagates a given input through all encapsulated layers and returns the final prediction of the network, whereas the `backward` function propagates a given gradient through all layers in reversed order. `zero_gradients` and `update_parameters` invoke the respective functions of the encapsulated layers.\n",
    "\n",
    "**Task 3b:** Test your implementation analogous to task 2f but using the `Network` class to encapsulate the linear and sigmoid layer.\n",
    "\n",
    "**Task 4:** Train the network you just implemented using the dataloader and train function given above and the hyperparameter given below.\n",
    "\n",
    "**Task 5:** Come up with a more sophisticated network structure and adjust the hyperparameter in order to increase the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, n_in, n_out, initial_sigma=0.1):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "\n",
    "        self.w = initial_sigma * np.random.randn(n_out, n_in)\n",
    "        self.b = np.zeros((1, n_out))\n",
    "\n",
    "        self.zero_gradients()\n",
    "\n",
    "        self.x_in = None\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        # ----- Add code for task 2a between comments -----\n",
    "        # -------------------------------------------------\n",
    "\n",
    "\n",
    "        return x_out\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        # ----- Add code for task 2d between comments -----\n",
    "        # -------------------------------------------------\n",
    "    \n",
    "        return self.d_in\n",
    "\n",
    "    def zero_gradients(self):\n",
    "        self.dw = np.zeros((self.n_out, self.n_in))\n",
    "        self.db = np.zeros((1, self.n_out))\n",
    "        self.dx = np.empty((0, self.n_in))\n",
    "\n",
    "    def update_parameters(self, learning_rate):\n",
    "        # ----- Add code for task 2e between comments -----\n",
    "        # -------------------------------------------------\n",
    "\n",
    "\n",
    "class MeanSquareError(Cost):\n",
    "    def __init__(self):\n",
    "        self.prediction = None\n",
    "        self.target = None\n",
    "\n",
    "    def estimate(self, target, prediction):\n",
    "        # ----- add code for task 2b between comments -----\n",
    "        # -------------------------------------------------\n",
    "    \n",
    "        return cost\n",
    "\n",
    "    def gradient(self):\n",
    "        # ----- add code for task 2c between comments -----\n",
    "        # -------------------------------------------------\n",
    "\n",
    "        return gradient\n",
    "\n",
    "\n",
    "class Network(Layer):\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    # ----- add code for task 3a between comments -----\n",
    "    # -------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "input_size = 28**2\n",
    "label_size = 10\n",
    "batch_size = 600\n",
    "learning_rate = 0.0001\n",
    "number_epochs = 100\n",
    "\n",
    "random_input = np.random.rand(batch_size, input_size)\n",
    "random_label = np.random.rand(batch_size, label_size)\n",
    "\n",
    "linear_layer = Linear(input_size, label_size)\n",
    "sigmoid_layer = Sigmoid()\n",
    "cost_function = MeanSquareError()\n",
    "\n",
    "# ----- add code for task 2f between comments -----\n",
    "# -------------------------------------------------\n",
    "\n",
    "# ----- add code for task 3b between comments -----\n",
    "# -------------------------------------------------\n",
    "\n",
    "# ----- add code for task 4 between comments ------\n",
    "# -------------------------------------------------\n",
    "\n",
    "# ----- add code for task 5 between comments ------\n",
    "# -------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback\n",
    "\n",
    "Aaaaaand we're done üëèüèºüçª\n",
    "\n",
    "If you have any suggestions on how we could improve this session, please let us know in the following cell. What did you particularly like or dislike? Did you miss any contents?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your feedbacks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Tasks\n",
    "\n",
    "**Task 6a:** Implement the `forward` function for the `SoftMax` layer.\n",
    "\n",
    "**Task 6b:** Implement the `estimate` function for the `CrossEntropy` cost.\n",
    "\n",
    "**Task 6c:** Implement the `gradient` function for the `CrossEntropy` cost.\n",
    "\n",
    "**Task 6d:** Implement the `backward` function for the `SoftMax` layer.\n",
    "\n",
    "**Task 6e:** Test your implementation by setting up a network using the soft max layer and cross entropy cost in combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMax(Layer):\n",
    "    def __init__(self):\n",
    "        self.x_out = None\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        # ----- add code for task 6a between comments -----\n",
    "        # -------------------------------------------------\n",
    "        return self.x_out\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        # ----- add code for task 6d between comments -----\n",
    "        # -------------------------------------------------\n",
    "        return d_in\n",
    "\n",
    "\n",
    "class CrossEntropy(Cost):\n",
    "    def __init__(self):\n",
    "        self.x_in = None\n",
    "        self.target = None\n",
    "        self.eps = 1e-12\n",
    "\n",
    "    def estimate(self, target, x_in):\n",
    "        # ----- add code for task 6b between comments -----\n",
    "        # -------------------------------------------------\n",
    "        return cost\n",
    "\n",
    "    def gradient(self, d_out):\n",
    "        # ----- add code for task 6c between comments -----\n",
    "        # -------------------------------------------------\n",
    "        return gradient\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "input_size = 28**2\n",
    "label_size = 10\n",
    "batch_size = 600\n",
    "learning_rate = 0.00001\n",
    "number_epochs = 100\n",
    "\n",
    "# ----- add code for task 6e between comments -----\n",
    "# -------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgements\n",
    "\n",
    "Yann LeCun and Corinna Cortes hold the copyright of [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, which is a derivative work from original NIST datasets. MNIST dataset is made available under the terms of the [CC BY-SA 3.0 DEED](https://creativecommons.org/licenses/by-sa/3.0/) license. \n",
    "The libraries used are numpy, skimage, six.moves, os, matplotlib, tqdm, etc. \n",
    "\n",
    "Contributor(s): Yuli Wu, Leon Weninger, Raphael Kolk."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
