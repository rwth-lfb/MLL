{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Machine Learning Laboratory\n",
    "\n",
    "# Session 4: CNN for Classification\n",
    "\n",
    "by Leon Weninger, modified by Emil Mededovic\n",
    "\n",
    "## Goal of this session\n",
    "\n",
    "In this session you will:\n",
    "* implement two deep neural networks\n",
    "* learn about different layer types and activation functions\n",
    "* experiment with visualization techniques\n",
    "\n",
    "Mind that there are still a few things we hide behind the scenes:\n",
    "* Data Loading / Handling is done using framework utilities (see additional pytorch doc)\n",
    "* Data Augmentation will be covered in a later session\n",
    "\n",
    "### Dataset\n",
    "\n",
    "This session uses the PascalVOC dataset, which is accessible on this server.\n",
    "The dataset comprises input images that can be classified with different strategies.\n",
    "We consider an image classification problem, i.e. decide which objects from a finite set of classes appear in the input image.\n",
    "Since multiple objects may appear in each image, this is a multi-label classification task.\n",
    "\n",
    "Let's dive right into the task:\n",
    "Make sure to run the imports and continue loading the dataset to RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "import torchvision.transforms as transforms\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.error import URLError\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define if the network should run on GPU or CPU. On the official tutorial machines, a GPU is available. Remember later on to specify the network to run on \"device\".\n",
    "\"cuda\" should be printed if a GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code should be skipped in lab as the dataset is already downloaded and only revisited if not done during lab!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the dataset\n",
    "\n",
    "We prepared a loader for you that can automatically grab a training- and testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets.voc import DATASET_YEAR_DICT\n",
    "\n",
    "DATASET_YEAR_DICT[\"2012\"][\"url\"] = (\n",
    "    \"https://data.brainchip.com/dataset-mirror/voc/VOCtrainval_11-May-2012.tar\"\n",
    ")\n",
    "dset_path = './dataset'\n",
    "# set download to True if dataset is not already downloaded\n",
    "voc_train = VOCSegmentation(root=dset_path, year=\"2012\", download=False, image_set=\"train\")\n",
    "voc_val = VOCSegmentation(root=dset_path, year=\"2012\", download=False, image_set=\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple lookup table from numerical values to names of the objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_label = {0: 'background', 1:'aeroplane', 2:'bicycle', 3:'bird', 4:'boat', 5:'bottle', 6:'bus', 7:'car', 8:'cat', 9:'chair', 10:'cow', 11:'diningtable',\n",
    "12:'dog', 13:'horse', 14:'motorbike', 15:'person', 16:'potted plant', 17:'sheep', 18:'sofa', 19:'train', 20:'tv/monitor'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below a few times to get an overview of the data available.\n",
    "The labels assigned to the classes are shown above the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(2, 1, figsize=(15,15))\n",
    "for i in range(2):\n",
    "    img, mask = voc_train[np.random.randint(0,voc_train.__len__() + 1)]\n",
    "    plt.subplot(1,2,i+1)\n",
    "    plt.imshow(img)\n",
    "    numerical_labels = np.unique(mask)\n",
    "    plt.title([class_to_label[i] for i in numerical_labels[1:-1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a network\n",
    "\n",
    "### Defining functional units\n",
    "\n",
    "As introduced in the preparation, we are going to implement a VGGNet model as a classifier.\n",
    "From the previous session you know the setup of classical Multilayer Perceptrons.\n",
    "\n",
    "**Q1a:** Which other layer types appear in the VGG architecture?\n",
    "\n",
    "The VGG architecture has a rather simple structure\n",
    "\n",
    "**Q1b:** How can a VGG net be structured in small functional units (3-4 layers)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ... write your answers/ideas in this box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blocks of the VGGNet should look like this:\n",
    "![Blockstructure](vgg16block-structure.png \"Blockstructure\")\n",
    "Note that this structure is a little bit different from the original architecture. and reflects some recent advances in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK** Implement the class below.\n",
    "\n",
    "*Hint:* implement a functional block by defining a new python class which inherits from nn.Module (see pytorch doc).\n",
    "You have to write an *init* and *forward* method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGBlock(nn.Module):\n",
    "    def __init__(self, ifeat, ofeat, N=2):\n",
    "        super(VGGBlock, self).__init__()\n",
    "        assert(N in (2, 3))\n",
    "        # Your code here:\n",
    "        # setup all layers inside a VGG block here\n",
    "        \n",
    "       \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Your code here:\n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Architecture\n",
    "\n",
    "The standard VGG Net is used to predict a single class. Recall that in PascalVOC multiple objects may be present in an image.\n",
    "\n",
    "**Q2:** What changes between a single and multi-label scenario?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ... write your answers/ideas in this box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of classes in PascalVOC is much smaller than in the ILSCVR Challenge (where VGG16 was benchmarked) the number of parameters for the Linear Layers can drastically be reduced in this session. Use _1024_ instead of _4096_ parameters.\n",
    "\n",
    "**TASK** Using the block diagram in the preparation and your pytorch module above, implement a VGG-16 network as nn.Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16, self).__init__()\n",
    "        # Your code here:\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Your code here:\n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "This part covers necessary preparations to use the images from PascalVOC in the training process.\n",
    "Read through the implementations below and using the pytorch doc explore what is done here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToLabel:\n",
    "    def __call__(self, pic):\n",
    "        img = np.array(pic, np.uint8, copy=False)\n",
    "        labels = np.unique(img)[1:-1]\n",
    "        one_hot_targets = np.zeros(20)\n",
    "        one_hot_targets[labels-1] = 1\n",
    "        # we need to subtract one from labels, so that labels reaches from 0 to 19 (not from 1 to 20 as originally)\n",
    "        return torch.from_numpy(one_hot_targets).float()\n",
    "\n",
    "mean_std = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "inp_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(*mean_std)\n",
    "])\n",
    "\n",
    "tgt_transform = transforms.Compose([\n",
    "    ToLabel(),\n",
    "]\n",
    ")\n",
    "\n",
    "train_set = VOCSegmentation(root=dset_path, image_set='train', transform=inp_transform, target_transform=tgt_transform)\n",
    "valid_set = VOCSegmentation(root=dset_path, image_set='val', transform=inp_transform, target_transform=tgt_transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=8, num_workers=1, shuffle=True)\n",
    "val_loader = DataLoader(valid_set, batch_size=32, num_workers=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "**Q3:** How is the training process structured? Which steps form an epoch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ... write your answers/ideas in this box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK** Implement a training loop for the VGG16 model.\n",
    "In PyTorch you need to set up model, loss function and optimizer. This is done as initialization before entering the loop.\n",
    "During trainig we iterate multiple times through the dataset until the loss is not reduced any further.\n",
    "Iteration in mini-batches is necessary since using the entire dataset at once would largely exceed the GPUs memory capacity.\n",
    "\n",
    "For an improved estimation of the actual performance we iterate over the validation data as well (*Recap* session on unbiased evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = VGG16()\n",
    "\n",
    "\n",
    "# send the network to the GPU\n",
    "# initialize your loss criterion\n",
    "# initialize an optimizer object\n",
    "# Your code here:\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(3):\n",
    "    # We use only 3 epochs for educational reasons as we don't have enough time to wait for a full training\n",
    "    ep_train_losses = []\n",
    "    \n",
    "    # Init progress bar\n",
    "    loop = tqdm.tqdm(enumerate(train_loader), total=len(train_loader), leave=True)\n",
    "    \n",
    "    for batch_nr, data in loop:\n",
    "        # get the required data and labels and wrap them in variables for the GPU processing\n",
    "        # ...\n",
    "        \n",
    "        # (optional) write some visualization to check if it works\n",
    "        # ...\n",
    "        \n",
    "        # compute a loss from the network output\n",
    "        # ...\n",
    "        \n",
    "        # ...  # zero old gradient values\n",
    "        # ...  # compute new gradients\n",
    "        # ...  # update weights\n",
    "        \n",
    "        # write some updating output for the current loss\n",
    "        # ...\n",
    "        \n",
    "        # Your code here:\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        ep_valid_losses = []\n",
    "        for batch_nr, data in enumerate(val_loader):\n",
    "            # Your code here:\n",
    "            \n",
    "            \n",
    "\n",
    "# save your trained model if you want to (Optional)\n",
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize some examples\n",
    "\n",
    "Verification is important! Let's first visualize some examples, so that we can see how good the network performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the quality of the model we have to find suitable measures to quantify the results for our test and validation data (ideally in a way we can easily understand), but more often than not, it will help tremendously to simply browse through some examples and verify the model by manual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing outputs\n",
    "visualize_loader = DataLoader(train_set, batch_size=1, num_workers=0, shuffle=True)\n",
    "l_iter = iter(visualize_loader)\n",
    "\n",
    "def lbls2names(nums):\n",
    "    nums = lbls\n",
    "    nums = nums.numpy().squeeze()\n",
    "    labels_int = np.where(nums>0.5)\n",
    "    labels_int = np.squeeze(np.asarray(labels_int), axis=0)\n",
    "\n",
    "    names = \"[\"\n",
    "    for l in labels_int:\n",
    "        names = names + class_to_label[l+1] + \", \"\n",
    "    names+=\"]\"\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, lbls = next(l_iter)  \n",
    "img_rgb = ((np.moveaxis(img.numpy().squeeze(),0,2))*mean_std[1]+mean_std[0])\n",
    "plt.imshow(np.clip(img_rgb,0,1))\n",
    "plt.show()\n",
    "lbls_str = lbls2names(lbls)\n",
    "print(\"Labels = \" + lbls_str)\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = img.to(device)\n",
    "    res = torch.sigmoid(network(inputs))\n",
    "    # Remember the sigmoid function at the end of the network\n",
    "\n",
    "#Show top5 and top1 predicted labels\n",
    "prediction = np.squeeze(res.cpu().numpy())\n",
    "top1 = np.argsort(prediction)[-1]\n",
    "top5 = np.argsort(prediction)[-5:]\n",
    "top5_string = \"[\"\n",
    "for i in top5:\n",
    "    top5_string = top5_string + class_to_label[i+1] + \", \"\n",
    "top5_string+=\"]\"\n",
    "print(\"Top5 = \" + top5_string)\n",
    "print(\"Top1 = \" + class_to_label[top1+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-N Accuracy\n",
    "\n",
    "We now want to quantify our accuracy.  \n",
    "Implement a Top-5 error measure, i.e. we score whenever a label we expect from the ground-truth appears in our Top-5 predictions.  \n",
    "Implement also a Top-1 error score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_scores = []\n",
    "top_1_scores = []\n",
    "for img_nr, data in enumerate(val_loader):\n",
    "    imgs, lbls = data\n",
    "    \n",
    "    network.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = imgs.to(device)\n",
    "        output = torch.sigmoid(network(inputs))\n",
    "    prediction = np.squeeze(output.cpu().numpy())\n",
    "    top5 = np.argsort(prediction, axis=1)[:,-5:]\n",
    "    top1 = np.argsort(prediction, axis=1)[:,-1]\n",
    "    \n",
    "    labels = lbls.numpy()\n",
    "        \n",
    "    # implementation top5 error\n",
    "    # Your code here:\n",
    "        \n",
    "\n",
    "# Print your error measures\n",
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statement:** The Top-5 error is a fair measure for this evaluation.\n",
    "\n",
    "**Q4:** Think, discuss, reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's think about an interesting question: Can we do better? What happens if we make our model deeper? Does this help our model learn more complex things? The answer is both Yes and No. Simply making the model deeper has often led to worse results. But, by adding something called skip connections, we can get the best of both worlds. This lets the model decide for itself how deep it should be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's explore the concept of the residual block (source: https://nn.labml.ai/resnet/index.html):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Blockstructure](ResnetBasicBlock.png \"Blockstructure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build First the Basic Block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicResnetBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int):\n",
    "        super(BasicResnetBlock, self).__init__()\n",
    "        # Your code here: \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Student code here \n",
    "        # Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resnet 18 architecture (source: Jason Brown et al.: CNN Based Image Classification of Malicious UAVs) should look like this:\n",
    "![Blockstructure](Resnet-18.png \"Blockstructure\")\n",
    "Now that we have the Basic Block, it is time to implement the whole Resnet 18 architecture ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class resnet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Your code here: \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Your code here: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train now the resnet18 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- students version ----- #\n",
    "# Your code here: \n",
    "           \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-N Accuracy (Resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- students version ----- #\n",
    "# Your code here: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hope you enjoyed the session! - Feel free to give your feedback and help us improve this lab..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback Cell:\n",
    "I hope this Notebook gave you a good start into python and Machine Learning. Let us know how you liked it. Any suggestions/ criticism are also welcome!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The libraries used are torch, torchvision, numpy etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
