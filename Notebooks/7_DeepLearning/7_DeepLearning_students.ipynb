{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Machine Learning\n",
    "\n",
    "## Session 7: Deep Learning\n",
    "\n",
    "by Leon Weninger\n",
    "\n",
    "### Goal of this session\n",
    "\n",
    "In this session you will:\n",
    "* implement a deep neural network\n",
    "* learn about different layer types and activation functions\n",
    "* experiment with visualization techniques\n",
    "\n",
    "Mind that there are still a few things we hide behind the scenes:\n",
    "* Data Loading / Handling is done using framework utilities (see additional pytorch doc)\n",
    "* Data Augmentation will be covered in a later session\n",
    "\n",
    "### Dataset\n",
    "\n",
    "This session uses the PascalVOC dataset, which is accessible on this server.\n",
    "The dataset comprises input images that can be classified with different strategies.\n",
    "We consider an image classification problem, i.e. decide which objects from a finite set of classes appear in the input image.\n",
    "Since multiple objects may appear in each image, this is a multi-label classification task.\n",
    "\n",
    "Let's dive right into the task:\n",
    "Make sure to run the imports and continue loading the dataset to RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.error import URLError\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define if the network should run on GPU or CPU. On the official tutorial machines, a GPU is available. Remember later on to specify the network to run on \"device\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the dataset\n",
    "\n",
    "We prepared a loader for you that can automatically grab a training- and testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_path = os.path.join(os.path.abspath(os.environ[\"HOME\"]), 'datasets')\n",
    "try:\n",
    "    voc_train = VOCSegmentation(root=dset_path, download=False, image_set=\"train\")\n",
    "    voc_val = VOCSegmentation(root=dset_path, download=False, image_set=\"val\")\n",
    "except RuntimeError:\n",
    "    if not os.path.isdir(dset_path):\n",
    "            os.makedirs(dset_path)\n",
    "    try:\n",
    "        # try with original host first, which however is often down\n",
    "        VOCSegmentation(root=dset_path, download=True)\n",
    "    except URLError:\n",
    "        # original host not available, falling to backup host & manual extraction\n",
    "        from torchvision.datasets.utils import download_and_extract_archive\n",
    "        from torchvision.datasets.voc import DATASET_YEAR_DICT\n",
    "        voc_2012 = DATASET_YEAR_DICT[\"2012\"]\n",
    "        voc_2012[\"url\"] = \"http://pjreddie.com/media/files/VOCtrainval_11-May-2012.tar\"\n",
    "        download_and_extract_archive(voc_2012[\"url\"], dset_path, filename=voc_2012[\"filename\"], md5=voc_2012[\"md5\"])\n",
    "    voc_train = VOCSegmentation(root=dset_path, download=False, image_set=\"train\")\n",
    "    voc_val = VOCSegmentation(root=dset_path, download=False, image_set=\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple lookup table from numerical values to names of the objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_label = {0: 'background', 1:'aeroplane', 2:'bicycle', 3:'bird', 4:'boat', 5:'bottle', 6:'bus', 7:'car', 8:'cat', 9:'chair', 10:'cow', 11:'diningtable',\n",
    "12:'dog', 13:'horse', 14:'motorbike', 15:'person', 16:'potted plant', 17:'sheep', 18:'sofa', 19:'train', 20:'tv/monitor'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below a few times to get an overview of the data available.\n",
    "The labels assigned to the classes are shown above the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(2, 1, figsize=(15,15))\n",
    "for i in range(2):\n",
    "    img, mask = voc_train[np.random.randint(0,voc_train.__len__() + 1)]\n",
    "    plt.subplot(1,2,i+1)\n",
    "    plt.imshow(img)\n",
    "    numerical_labels = np.unique(mask)\n",
    "    plt.title([class_to_label[i] for i in numerical_labels[1:-1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a network\n",
    "\n",
    "### Defining functional units\n",
    "\n",
    "As introduced in the preparation, we are going to implement a VGGNet model as a classifier.\n",
    "From the previous session you know the setup of classical Multilayer Perceptrons.\n",
    "\n",
    "**Q1a:** Which other layer types appear in the VGG architecture?\n",
    "\n",
    "The VGG architecture has a rather simple structure\n",
    "\n",
    "**Q1b:** How can a VGG net be structured in small functional units (3-4 layers)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ... write your answers/ideas in this box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blocks of the VGGNet should look like this:\n",
    "![Blockstructure](vgg16block-structure.png \"Blockstructure\")\n",
    "Note that this structure is a little bit different from the original architecture. and reflects some recent advances in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK** Implement the class below.\n",
    "\n",
    "*Hint:* implement a functional block by defining a new python class which inherits from nn.Module (see pytorch doc).\n",
    "You have to write an *init* and *forward* method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGBlock(nn.Module):\n",
    "    def __init__(self, ifeat, ofeat, N=2):\n",
    "        super(VGGBlock, self).__init__()\n",
    "        assert(N in (2, 3))\n",
    "        \n",
    "        # setup all layers inside a VGG block here\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # define the forward method\n",
    "        # keep in mind that some blocks have 2 sub-blocks, others 3. This should be decided based on the value of N\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Architecture\n",
    "\n",
    "The standard VGG Net is used to predict a single class. Recall that in PascalVOC multiple objects may be present in an image.\n",
    "\n",
    "**Q2:** What changes between a single and multi-label scenario?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ... write your answers/ideas in this box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of classes in PascalVOC is much smaller than in the ILSCVR Challenge (where VGG16 was benchmarked) the number of parameters for the Linear Layers can drastically be reduced in this session. Use _1024_ instead of _4096_ parameters.\n",
    "\n",
    "**TASK** Using the block diagram in the preparation and your pytorch module above, implement a VGG-16 network as nn.Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16, self).__init__()\n",
    "        \n",
    "        # set up the blocks for the feature extractor part\n",
    "        self.block1 = VGGBlock(3, 64)\n",
    "        self.block2 = \"\"# ... your part\n",
    "        # ...\n",
    "        \n",
    "        k = 0 # size of the feature plane after the extractor\n",
    "        \n",
    "        # set up the dense layers here, this is the classifier part of the network\n",
    "        # don't forget the Dropout for a better learning behaviour\n",
    "        # ... your part\n",
    "\n",
    "    def forward(self, x):\n",
    "        # implement the forward function\n",
    "        \n",
    "        # print(x.size())  # useful for finding the 'k' above\n",
    "        # x = torch.flatten(x, 1)  # this call transforms the 2D feature field into a vector\n",
    "\n",
    "        # implement the classifier function\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "This part covers necessary preparations to use the images from PascalVOC in the training process.\n",
    "Read through the implementations below and using the pytorch doc explore what is done here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToLabel(object):\n",
    "    def __call__(self, pic):\n",
    "        img = np.array(pic, np.uint8, copy=False)\n",
    "        labels = np.unique(img)[1:-1]\n",
    "        one_hot_targets = np.zeros(20)\n",
    "        one_hot_targets[labels-1] = 1\n",
    "        # we need to subtract one from labels, so that labels reaches from 0 to 19 (not from 1 to 20 as originally)\n",
    "        return torch.from_numpy(one_hot_targets).float()\n",
    "\n",
    "mean_std = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "inp_transform = transforms.Compose([\n",
    "    transforms.Resize((320,320)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(*mean_std)\n",
    "])\n",
    "\n",
    "tgt_transform = transforms.Compose([\n",
    "    ToLabel(),\n",
    "]\n",
    ")\n",
    "\n",
    "train_set = VOCSegmentation(root=dset_path, image_set='train', transform=inp_transform, target_transform=tgt_transform)\n",
    "valid_set = VOCSegmentation(root=dset_path, image_set='val', transform=inp_transform, target_transform=tgt_transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=8, num_workers=0, shuffle=True)\n",
    "val_loader = DataLoader(valid_set, batch_size=32, num_workers=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "**Q3:** How is the training process structured? Which steps form an epoch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ... write your answers/ideas in this box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK** Implement a training loop for the VGG16 model.\n",
    "In PyTorch you need to set up model, loss function and optimizer. This is done as initialization before entering the loop.\n",
    "During trainig we iterate multiple times through the dataset until the loss is not reduced any further.\n",
    "Iteration in mini-batches is necessary since using the entire dataset at once would largely exceed the GPUs memory capacity.\n",
    "\n",
    "For an improved estimation of the actual performance we iterate over the validation data as well (*Recap* session on unbiased evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = VGG16()\n",
    "\n",
    "# send the network to the GPU\n",
    "# initialize your loss criterion\n",
    "# initialize an optimizer object\n",
    "\n",
    "for epoch in range(3):\n",
    "    \n",
    "    ep_train_losses = []\n",
    "    for batch_nr, data in enumerate(train_loader):\n",
    "        # get the required data and labels and wrap them in variables for the GPU processing\n",
    "        # ...\n",
    "        \n",
    "        # (optional) write some visualization to check if it works\n",
    "        # ...\n",
    "        \n",
    "        # compute a loss from the network output\n",
    "        # ...\n",
    "        \n",
    "        # ...  # zero old gradient values\n",
    "        # ...  # compute new gradients\n",
    "        # ...  # update weights\n",
    "        \n",
    "        # write some updating output for the current loss\n",
    "        # ...\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        ep_valid_losses = []\n",
    "        for batch_nr, data in enumerate(val_loader):\n",
    "            # repeat the steps above for the validation set\n",
    "            # which steps have to be skipped?\n",
    "\n",
    "\n",
    "# save your trained model if you want to\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize some examples\n",
    "\n",
    "Verification is important! Let's first visualize some examples, so that we can see how good the network performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the quality of the model we have to find suitable measures to quantify the results for our test and validation data (ideally in a way we can easily understand), but more often than not, it will help tremendously to simply browse through some examples and verify the model by manual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing outputs\n",
    "visualize_loader = DataLoader(train_set, batch_size=1, num_workers=0, shuffle=True)\n",
    "l_iter = iter(visualize_loader)\n",
    "\n",
    "def lbls2names(nums):\n",
    "    nums = lbls\n",
    "    nums = nums.numpy().squeeze()\n",
    "    labels_int = np.where(nums>0.5)\n",
    "    labels_int = np.squeeze(np.asarray(labels_int), axis=0)\n",
    "\n",
    "    names = \"[\"\n",
    "    for l in labels_int:\n",
    "        names = names + class_to_label[l+1] + \", \"\n",
    "    names+=\"]\"\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, lbls = next(l_iter)  \n",
    "img_rgb = ((np.moveaxis(img.numpy().squeeze(),0,2))*mean_std[1]+mean_std[0])\n",
    "plt.imshow(np.clip(img_rgb,0,1))\n",
    "plt.show()\n",
    "lbls_str = lbls2names(lbls)\n",
    "print(\"Labels = \" + lbls_str)\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = img.to(device)\n",
    "    res = torch.sigmoid(network(inputs))\n",
    "    # Remember the sigmoid function at the end of the network\n",
    "\n",
    "#Show top5 and top1 predicted labels\n",
    "prediction = np.squeeze(res.cpu().numpy())\n",
    "top1 = np.argsort(prediction)[-1]\n",
    "top5 = np.argsort(prediction)[-5:]\n",
    "top5_string = \"[\"\n",
    "for i in top5:\n",
    "    top5_string = top5_string + class_to_label[i+1] + \", \"\n",
    "top5_string+=\"]\"\n",
    "print(\"Top5 = \" + top5_string)\n",
    "print(\"Top1 = \" + class_to_label[top1+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-N Accuracy\n",
    "\n",
    "We now want to quantify our accuracy.  \n",
    "Implement a Top-5 error measure, i.e. we score whenever a label we expect from the ground-truth appears in our Top-5 predictions.  \n",
    "Implement also a Top-1 error score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_scores = []\n",
    "top_1_scores = []\n",
    "for img_nr, data in enumerate(val_loader):\n",
    "    imgs, lbls = data\n",
    "    \n",
    "    network.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = imgs.to(device)\n",
    "        output = torch.sigmoid(network(inputs))\n",
    "    prediction = np.squeeze(output.cpu().numpy())\n",
    "    top5 = np.argsort(prediction, axis=1)[:,-5:]\n",
    "    top1 = np.argsort(prediction, axis=1)[:,-1]\n",
    "    \n",
    "    labels = lbls.numpy()\n",
    "        \n",
    "    # implementation top5 error\n",
    "\n",
    "    \n",
    "    # Implement your on error measure\n",
    "    # ...\n",
    "\n",
    "        \n",
    "# Print your error measures\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statement:** The Top-5 error is a fair measure for this evaluation.\n",
    "\n",
    "**Q4:** Think, discuss, reason.\n",
    "\n",
    "**Q5:** Suggest and implement your own idea to compute a human interpretable score for the network performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ... write your answers/ideas in this box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hope you enjoyed the session! - Feel free to give your feedback and help us improve this lab..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ... your ideas for a improvements, peace and a better world in general, here pls :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
