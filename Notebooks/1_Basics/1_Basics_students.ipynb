{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Laboratory\n",
    "Institute of Imaging and Computer Vision, RWTH Aachen\n",
    "\n",
    "Version SS2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 1: Basics All at Once\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals of this session\n",
    "\n",
    "After this session, you will have an understanding of:\n",
    "\n",
    "First 30min:\n",
    "-  basic python programming\n",
    "-  python datastructures\n",
    "-  some important python packages: `NumPy`, `matplotlib.pyplot`, `scikit-learn`\n",
    "-  working with images\n",
    "-  some common terms used in ML \n",
    "\n",
    "\n",
    "Second 30min:\n",
    "-  the Iris dataset \n",
    "-  PCA\n",
    "\n",
    "Third 30min:\n",
    "- Difference between supervised and unsupervised learning\n",
    "- What is meant by Classification in ML\n",
    "- Supervised Learning:\n",
    "    - Support Vector Machine (SVM)\n",
    "    - Optional: k-nearest neighbour\n",
    "\n",
    "Fourth 30min:\n",
    "- Unsupervised Learning:\n",
    "    - k-means clustering\n",
    "    - GMM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**>> Time Management**\n",
    "  \n",
    "**>> Please keep in mind the suggested 30min intervals. Not as fast as expected? No problem at all: you will have ca. 60min buffer time. Still not completed? You can continue with the last piece at home.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure of the Notebook: \n",
    "You need to perform the tasks and answer the questions in the notebook.\n",
    "For most of the tasks you will find either examples or hints to help you.\n",
    "For this notebook the hints are mostly in the form of commands that you will be required to use. \n",
    "Feel free to search the internet and find out how to use the required command, eg. which inputs need to provided and how many outputs are to be expected, etc.\n",
    "\n",
    "Let's start with some basic python programming.\n",
    "Have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python requires you to `import` the package you want to work on. Let's `import` an important package that motivates us to learn python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary packages\n",
    "import antigravity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic `NumPy` operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary packages\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Create a 1D array with 10 elements \n",
    "Hint: you may use any of `NumPy` built-in array generators, eg. `np.zeros`, `np.ones`, `np.arange`, `np.random`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Reshape the above array (`np.reshape`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Create another 1D array and combine it with the previous array (a) vertically (b) horizontally\n",
    "(eg. using `np.concatenate`, `np.vstack`, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Get positions where first and second array have the same elements (eg. `np.where`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Extract the following from the matrix you created by horizontal concatenation:\n",
    " (a) the first element of the matrix, (b) the first row, (c) the first column,\n",
    " (d) any subset of your choice, eg.the four center elements.\n",
    "\n",
    "#### This is called 'Slicing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Python data types\n",
    "- numbers (eg. int, float32)\n",
    "- booleans (True, False)\n",
    "- 'null' type (None)\n",
    "- strings (eg. 'hello', 'world')\n",
    "- lists (eg. [8, 'hello', 5>7])\n",
    "- tuples (eg. (8,'hello',5>7))\n",
    "\n",
    "Hopefully you are already familiar with the common datatypes. We are going to discuss the last two:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List\n",
    "Python lists allow you to store a sequence of different objects, as in the above example, [int, string, bool]. \n",
    "\n",
    "Note: Lists are created using sqaure brackets [ ] and comma separated values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks on List**\n",
    "1. Create a list containing a string, a bool, and an int\n",
    "2. Change any one object of your choice from the list\n",
    "3. Add new objects to the end of the existing list using (a) `append` (b) `extend`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What is the difference between `append` and `extend`?\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks on List (cont.)**\n",
    "5. Try out the examples using the functions: `pop`, `remove`, and `del`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myList = [0,5,3,4,3]\n",
    "# myList.pop(3)\n",
    "\n",
    "# myList = [0,5,3,4,3]\n",
    "# myList.remove(3)\n",
    "# myList\n",
    "\n",
    "# myList = [0,5,3,4,3]\n",
    "# del myList[3]\n",
    "# myList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What is the difference among functions from task 5? \n",
    "\n",
    "Answer: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuple\n",
    "Similar to list, tuples can also store a sequence of arbitrary objects. They are constructed using parentheses.\n",
    "\n",
    "Important: List can be changed after construction (mutable). Tuple cannot be changed (immutable). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks on Tuple**\n",
    "1. Create a tuple object and replace the first object with the int object 0\n",
    "2. Convert the above tuple to list, change the first object, convert back to tuple\n",
    "\n",
    "Hint: Use functions `list()` and `tuple()` to convert datatype \n",
    "Tip: You can check datatype using either `type()` or `isinstance(var,dtype)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence types\n",
    "- list\n",
    "- string\n",
    "- tuple\n",
    "- `NumPy` array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task on sequence types** (handy when programming in python!)\n",
    "\n",
    "- Checking if object is contained within sequence or not.\n",
    "\n",
    "Understand the following example code and then perform the given tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code:\n",
    "x = (1,2,3)\n",
    "y1 = 3 in x\n",
    "y2 = 5 not in x\n",
    "print('y1 =',y1)\n",
    "print('y2 =',y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Perform the tasks according to the instructions in the following three cells and answer the question at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given:\n",
    "task1 = 'let us learn python'\n",
    "# Task: Check if 'us' is present in task1 or not\n",
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given:\n",
    "task2 = [10,20,30,40]\n",
    "# Task: Check if [10,20] is present in task2 or not\n",
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given:\n",
    "task3 = [True,[2,3],'hello']\n",
    "# Task: Check if [2,3] is present in task3 or not\n",
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Explain the different outputs in task 2 and task 3 above.\n",
    "\n",
    "Answer: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Working with images\n",
    "We will learn to load, display and manipulate images using the `matplotlib` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary packages\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "irisImg = cv2.imread('/home/praktikum/MLlab/irisImage.png') \n",
    "irisImg = cv2.cvtColor(irisImg, cv2.COLOR_BGR2RGB) # because cv2 reads images as bgr!\n",
    "# Display  image\n",
    "plt.imshow(irisImg) # display in RGB\n",
    "plt.show()\n",
    "irisImg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Find out the datatype of the image.\n",
    "Hint: You've already used this command above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use the `NumPy` manipulations we learned to manipulate the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Extract and display only one of the species\n",
    "Hint: You can do so by the array slicing method you learned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# Please name the image 'iris1'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Delete the white border from all the sides of the extracted image (you can do it by simply cropping the image further down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# You may use plt.axis('off') to display images without the axis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some useful image manipulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Resize the image by three times (use `cv2.resize`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# Please name the image 'resizedImg'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Convert the image to grayscale. This can be done by taking the sum of weighted RGB values as follows: 0.2989*R + 0.5870*G + 0.1140*B\n",
    "\n",
    "Hint: Use slicing to extract individual channels\n",
    "\n",
    "Hint: Use `cmap='gray'` in `plt.imshow` to transorm into black&white mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# Please name your image 'irisGray'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could use the built-in function `cv2.COLOR_RGB2GRAY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# irisGray = cv2.cvtColor(iris1, cv2.COLOR_RGB2GRAY)\n",
    "# plt.imshow(irisGray, cmap='gray')\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Plot the histogram of the gray scale image (use `plt.hist()` with `ravel()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "In Machine Learning tasks, normalization is often the first step. It essentially means scaling and centering the data values for faster convergence and improved accuracy. Here we show one method of normalizing by subtracting the minimum value from all data points and then dividing by the range of values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization method 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization method 1: \n",
    "irisNorm1 = (irisGray-np.min(irisGray))/(np.max(irisGray)-np.min(irisGray))\n",
    "plt.imshow(irisNorm1, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "# To see the histogram of the normalized image\n",
    "plt.hist(irisNorm1.ravel(),32)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that after normalization the image is rescaled between 0 and 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization method 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Perform normalization on the grayscale image by subtracting the mean and scaling by the standard deviation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization method 2: \n",
    "# Your code here:\n",
    "# Please name your image 'irisNorm2'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're a little comfortable with python and handling different kinds of data. We now move on to the next step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Machine Learning\n",
    "In ML, we try to create a model based on the data we have at hand. So the first most important thing is to learn to represent that data such that the computer understands it. In our course, we will use python's Scikit-Learn package.\n",
    "\n",
    "### The dataset\n",
    "The Iris flower dataset (or Fisher's or Anderson's Iris data set) is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper. The aim was to quantify the morphologic variation of Iris flowers of three related species (the three flowers we saw above!). \n",
    "\n",
    "- The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor).\n",
    "- Four features were measured from each sample: the length and the width of the sepals and petals, in centimetres. \n",
    "\n",
    "Based on the combination of these four features, Fisher developed a linear discriminant model to distinguish the species from each other.\n",
    "\n",
    "We will analyze and visualize the data using `seaborn`. It is a handy python package for visualising statistic data and it offers a plethora of nice plotting tools. It also has the aforementioned Iris dataset included. The Iris dataset in seaborn does not use `numpy` or `cv2`, as we don't work on the images directly. Instead, we work with 'features,' as mentioned above. This information is stored in a so called `pandas dataframe`, another useful tool for statistical analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now load this dataset and display the first few elements.\n",
    "# importing necessary packages\n",
    "import seaborn as sns  \n",
    "iris = sns.load_dataset('iris')\n",
    "iris.head() #displaying the data (partially)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: pandas dataframe displays the data in a very intuitive tabular format.\n",
    "Here we see the four features our data has as individual columns.\n",
    "The species on the last column are formed by all the features from the corresponding rows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "sns.set_theme()\n",
    "sns.pairplot(iris, hue='species',height=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Here we see a visual pair-wise comparison of all the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with our dataset using `sklearn`\n",
    "We will now load the data using the Python ML library `sklearn`.\n",
    "It includes many ML algorithms, some of which we will be learning in this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary packages\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the dataset\n",
    "print(iris.data)\n",
    "# Displaying the datatype\n",
    "print(type(iris.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "1. The dataset was loaded as a numpy array\n",
    "2. We do not see the species information here\n",
    "     \n",
    "In `sklearn`, the dataset can be separated into 'features' and 'targets' as follows (`sklearn` stores species information as targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = iris.data[:,[0,1,2,3]]\n",
    "targets = iris.target "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Extract all the features of the first flower species\n",
    "Hint: You will find some more information when you inspect the shape of 'features' (and 'targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to visualize the relationship between some of the features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the relationship between the Sepal Length and Sepal Width\n",
    "    \n",
    "species = ('Iris-setosa', 'Iris-versicolor', 'Iris-virginica')\n",
    "colors = ('blue','green','red')\n",
    "\n",
    "data = [[features[np.where(targets == target)][:, feature] for feature in [0, 1]] for target in range(3)]\n",
    "\n",
    "for item, color, group in zip(data, colors, species):\n",
    "    plt.scatter(item[0], item[1], color=color)\n",
    "    plt.title('Iris dataset scatter plot')\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** In a similar manner, plot petal length versus petal width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** State your findings, eg. do these two features provide a better species separation? \n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have been comparing only two features at time only because it is easier to visualize. In ML, however, it is quite common to have hundreds of features. In such a case, we might have some features that are redundant by, for eg., being just a linear combination of some other features! Hence, it is common to perform dimensionality reduction to retain only the most representative features from the entire set. One way of doing is by Principal Component Analysis (PCA). Here we try to understand and implement PCA:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Summary:\n",
    "- Standardize the data.\n",
    "- Calculate Eigenvectors and Eigenvalues from the covariance matrix.\n",
    "- Sort eigenvalues in descending order\n",
    "- Choose the k eigenvectors that correspond to the k largest eigenvalues (k=number of dimensions of the new feature subspace (k≤d)).\n",
    "- Construct the projection matrix W from the selected k eigenvectors.\n",
    "- Transform the original dataset X via W to obtain a k-dimensional feature subspace Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features\n",
    "Y = targets\n",
    "# importing necessary packages for standardizing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Calculate the covariance matrix of the **transposed** feature matrix (use `NumPy`'s `cov` function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# Please name the matrix as 'cov_mat'\n",
    "# Remember to transpose the matrix before computing the covariance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Perform eigenvalue decomposition on cov_mat (use `NumPy`'s linear algebra function `eig`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# Please name the matrix as 'cov_mat'\n",
    "# Please call your variables eig_vecs and eig_vals\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**  How many PC values should we retain? \n",
    "\n",
    "**Answer:** I don't know either ;) \n",
    "\n",
    "But we can both find it out by calulating the 'Explained Variance.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = sum(eig_vals)\n",
    "var_exp = [(i/total)*100 for i in sorted(eig_vals, reverse=True)]\n",
    "\n",
    "for pos, data in enumerate(var_exp):\n",
    "    plt.bar(pos, data, align='edge')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows that around 73% of the variance is captured by the first PC and almost 23% by the second. We can safely ignore the third and the fourth component without losing much information. \n",
    "In principle, we are reducing the 4D feature space to a 2D feature subspace, by choosing the \"top 2\" eigenvectors with the highest eigenvalues to construct a d×k-dimensional eigenvector matrix W. So let's construct W."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eig_pairs.sort()\n",
    "eig_pairs.reverse()\n",
    "  \n",
    "matrix_w = np.hstack((eig_pairs[0][1].reshape(4,1), \n",
    "                      eig_pairs[1][1].reshape(4,1)))\n",
    "\n",
    "print('Matrix W:\\n', matrix_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we will use the 4×2-dimensional projection matrix W to transform our samples onto the new subspace via the equation\n",
    "Y=X×W, where Y is a 150×2 matrix of our transformed samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = X.dot(matrix_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = ('Iris-setosa', 'Iris-versicolor', 'Iris-virginica')\n",
    "colors = ('blue','green','red')\n",
    "data = [[Y[np.where(targets == target)][:, feature] for feature in [0, 1]] for target in range(3)]\n",
    "\n",
    "for item, color, group in zip(data, colors, species):\n",
    "    plt.scatter(item[0], item[1], color=color)\n",
    "    plt.title('Projection matrix')\n",
    "plt.xlabel('PCA1')\n",
    "plt.ylabel('PCA2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA using `scikit-learn`\n",
    "Now we can compare the self-implemented PCA with the one in `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary packages\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "\n",
    "sklearn_pca = sklearnPCA(n_components=2)\n",
    "features_PCA = sklearn_pca.fit_transform(X)\n",
    "\n",
    "species = ('Iris-setosa', 'Iris-versicolor', 'Iris-virginica')\n",
    "colors = ('blue','green','red')\n",
    "data = [[features_PCA[np.where(targets == target)][:, feature] for feature in [0, 1]] for target in range(3)]\n",
    "\n",
    "for item, color, group in zip(data, colors, species):\n",
    "    plt.scatter(item[0], item[1], color=color)\n",
    "    plt.title('Projection matrix')\n",
    "plt.xlabel('PCA1')\n",
    "plt.ylabel('PCA2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "What is the difference between LDA and PCA algorithms for dimension reduction in the term of annotation?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Supervised Learning (Classification of the Iris dataset)\n",
    "\n",
    "Classification tasks aim at predicting the class of a given data sample. For the Iris dataset, we want to predict which one of the three species a given sample belongs to. In supervised learning, we do so by 'learning' a model based on some training samples. We then use this trained model to make predictions on 'unseen' or test-dataset.\n",
    "\n",
    "Before we dive into any of the algorithms, we need to get introduced to the idea of splitting the datset into train and test sets. \n",
    "\n",
    "We know that the Iris dataset consists of 150 samples. Now, we will take 80% of the samples as our training set and the remaining as test set. We calculate the accuracy of our model on the test set. The common ML terminology is as follows: X_train, X_test are, respectively, the feature vectors for training and testing; and y_train and y_test are the corresponding labels for X_train and X_test. The predictions will be stored in y_pred.\n",
    "**Task:** Using the `train_test_split` function of `sklearn`, split the PCA features obtained above in accordance with the standard ML naming convention (as explained above). \n",
    "\n",
    "Tip: Please use the default parameters for the split. You will learn this topic more in details in the next session ;)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your code here:\n",
    "# Hint: import the necessary modules first\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 6.1 My very own classifier\n",
    "**Task:** Refer to the PCA plot above and design your own classifier (by completing the assisting code!). For this, you may define simple linear or non-linear conditions to classify the sample points into one of the three classes. \n",
    "\n",
    "Example condition: if PCA1 for given sample < -2, sample is Iris-Setosa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Complete the following lines of code: \n",
    "\n",
    "class StudentsClassifier:\n",
    "    def predict(self, X):\n",
    "        return np.array([self.predict_single(x) for x in X])\n",
    "    \n",
    "    def predict_single(self, X):\n",
    "        # X[0] = PCA feature 1\n",
    "        # X[1] = PCA feature 2\n",
    "        \n",
    "        # write a function that returns the index of the class\n",
    "        # e.g. if Iris-Setosa, return 0 \n",
    "        \n",
    "\n",
    "        # Your code here:\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Qualitative evaluation\n",
    "\n",
    "Below is a ready-to-use function to visualize classification boundaries. You are not required to understand it in detail but try to understand what inputs/outputs the functions require/produce. You will need to use these later in the notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    \"\"\"Create a mesh of points to plot in\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: data to base x-axis meshgrid on\n",
    "    y: data to base y-axis meshgrid on\n",
    "    h: stepsize for meshgrid, optional\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xx, yy : ndarray\n",
    "    \"\"\"\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "\n",
    "def plot_contours(clf, X, y, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    clf: a classifier\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(constrained_layout=True)\n",
    "    \n",
    "    X0, X1 = X[:, 0], X[:, 1]\n",
    "    xx, yy = make_meshgrid(X0, X1)\n",
    "    \n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.7)\n",
    "    \n",
    "    ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=30, edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the function $plot\\_contours$, visualize the training boundaries obtained by your classifier by providing the correct arguments to it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calling your classifier\n",
    "my_clf = StudentsClassifier()\n",
    "\n",
    "# Visualizing class boundaries for my_clf\n",
    "plot_contours(my_clf, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 6.3. Quantitative evaluation\n",
    "\n",
    "**Task:** First 'predict' the classes for X_test using the classifier you just defined above. And then calculate the accuracy on this test dataset. Use `scikit`'s `accuracy_score`.\n",
    "\n",
    "Hint: First import the necessary module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your code here:\n",
    "my_clf = StudentsClassifier()\n",
    "\n",
    "# y_pred = ? # Complete the code!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 6.4. Support Vector Machine\n",
    "\n",
    "SVMs are a powerful and flexible class of supervised algorithms for both classification and regression. They are memory efficient in that they use only a subset of the training points in the decision function (called support vectors). The simplest SVM uses a linear kernel to separate classes. \n",
    "**Task:** Using `sklearn`'s `svm.SVC`, implement a SVM with a linear kernel. Also perform a qualitative and a quantitative evaluation just as you did previously. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "# Example code: LinearSVC\n",
    "clf0 = svm.LinearSVC() # SVC = Support Vector Classifier\n",
    "# train clf0 on X_train and y_train\n",
    "clf0.fit(X_train, y_train)\n",
    "# visualize \n",
    "plot_contours(clf0, X_train, y_train)\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Question:** How is it in comparison to the your classifier? (Compare the accuracies).\n",
    "\n",
    "Answer: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 6.4. More on accuracy metrics\n",
    "\n",
    "The Confusion Matrix provides a better summary of the classification performance than just the accuracy score. The latter is often not the best measure for classification tasks involving more than two classes. Calculating a confusion matrix can give you a better idea of what your classification model is getting right and what types of errors it is making.\n",
    "**Task:** Read up on Confusion Matrix if you are not familiar with the term. Then using `sklearn`'s `confusion_matrix` draw it and try to understand the perfomance of your classifier. \n",
    "\n",
    "Tip: You may also want to look into `sklearn`'s `classification_report` function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stay tuned for more metrics in the next session ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5. Tuning the Hyperparameters\n",
    "\n",
    "There are several parameters that can help achieve better results (introduced in the Preparatory Material). \n",
    "\n",
    "- Kernel: Depending on the (expected) distribution of our classes, we can choose different types of functions, eg. linear, polynomial, and radial basis function (RBF). As might be obvious, the latter two are useful for non-linear hyperplane.\n",
    "\n",
    "- Regularization: `C` in scikit-learn is a penalty parameter that controls the flexibility allowed to the hyperplane. A smaller value of C creates a small-margin hyperplane and a larger value creates a larger-margin hyperplane. \n",
    "\n",
    "- Gamma: This defines how far the influence of a single training example reaches, with low values meaning ‘far’ and high values meaning ‘close’. A small gamma value define a Gaussian function with a large variance. In this case, two points can be considered similar even if are far from each other. In the other hand, a large gamma value means define a Gaussian function with a small variance and in this case, two points are considered similar just if they are close to each other.\n",
    "**Task:** With a simple trial and error approach try to visualize and understand the effect of the hyperparameters and find an optimum classifier for the given dataset. You may use the metrices you learnt about to evaluate the performance of the different classifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6. Optional: k-nearest neighbour (knn)\n",
    "#### *>> You first jump to 7. Unsupervised Learning and come back if you're interested.*\n",
    "#### *>> But they are very cooool!*\n",
    "### knn Algorithm\n",
    "\n",
    "- Define a distance metric (Euclidean distance)\n",
    "- Choose a value for k (= the number of nearest neighbours) \n",
    "- Take k-nearest neighbors of the new data point, according to your distance metric\n",
    "- Assign the new data point the same category as its nearest neighbors\n",
    "### 6.7. Optional: My very own knn classifier\n",
    "\n",
    "We define our own knn classifier according to the above algorithm. \n",
    "**Task (Step 1):** Define a function that calculates the euclidean distance between two points. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_ecd(v1, v2):\n",
    "    # Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task (Step 2):** Define a function that uses my_ecd to calculate the distance between one single test data point with *all* the training data points. Save the distances *along with their respective indices* in a list. Return the *sorted* list as the output of the function. \n",
    "\n",
    "Hint: 1. To get indices, you may want to use `enumerate`.        2. You may use the function `sort` or `sorted`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_distance_metric(X_train, single_test):\n",
    "    \"\"\"Calculates the distance between one test sample X_test and every sample in X_train.\n",
    "\n",
    "    Parameters:\n",
    "    X_train = all available training samples\n",
    "    single_test = one particular test sample\n",
    "    k = number of nearest neighbours\n",
    "    -----------\n",
    "    Returns: sorted distance list  \n",
    "    \"\"\"\n",
    "    dist_list = []\n",
    "    # Your code here:\n",
    "    # Define a for-loop to compute distance\n",
    "\n",
    "    return dist_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Taks (Step 3):** Define a function to save the first k target values corresponding to dist_list obtained above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_target_list(dist_list, y_train, k): \n",
    "    # Your code here:\n",
    "    # make a list of the k neighbors' targets\n",
    "\n",
    "\n",
    "    return target_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task (Step 4):** Define a function that assigns predictions to the test data points. \n",
    "\n",
    "Hint: Use `most_common`method from the `Counter` object to get the target that occurs maximum number of times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def my_predict(target_list):\n",
    "    # Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task (Step 5):** Finally define a function that loops through all data points predicting each one by one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_knn(X_test, X_train, y_train, k):    \n",
    "    all_predictions = []\n",
    "    # Your code here:\n",
    "    # define a for-loop to loop through all the test data points \n",
    "    # to get the predictions for each one of them individually \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Use the knn-function\n",
    "to predict X_test and calculate the accuracy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Task:** Use the built-in knn classifier from `scikit-learn`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How does your knn classifier perform in in comparison to the built-in function? \n",
    "\n",
    "Answer:\n",
    "\n",
    "\n",
    "## 7. Unsupervised Learning \n",
    "Until now we always worked with features and targets (labels) of the given dataset. In unsupervised learning, we do not have any labels for the data. For classification task, here, we will rely on some clustering algorithms. \n",
    "\n",
    "In this section, we will see the K-means and Gausssian Mixture Model (GMM) clustering methods. These agorithms require us to 'guess' how many clusters (classes) we have. \n",
    "### 7.1. K-means \n",
    "\n",
    "This is the simplest clustering algorithm. We initialize the algorithm with 'k' clusters according to which we get 'k' centroids. The algorithm then iteratively assigns every datapoint to its nearest cluster. The 'means' in its name refers to averaging of the data, i.e., finding the centroid.\n",
    "**Task:** Perform unsupervised classification using `sklearn's` `KMeans`. To check your results, print out the output lables and compare with the target values. \n",
    "\n",
    "Note: For this task we ignore the several optional parameters. Providing only the `n_clusters` argument will suffice for this task. \n",
    "**Question:** Before you begin the task, think about which dataset should you work with here? \n",
    "\n",
    "Hint: Unsupervised learning = NO labels available\n",
    "\n",
    "Answer: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Is it informative to compare the labels and the targets? Explain.\n",
    "\n",
    "Answer: \n",
    "\n",
    "\n",
    "### 7.2 Visualization of labels\n",
    "\n",
    "I hope by now it is clear to you why calculating accuracy as done previously does not make sense in this case. Hence, we perform a simple visualization to assess the performance. \n",
    "**Task:** Plot the results of the kmeans function as a scatter plot (similar to PCA). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "\n",
    "# Optional: Plotting the centroids of the clusters\n",
    "# plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], s = 70, c = 'black')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 The Elbow Method\n",
    "\n",
    "In a truly unsupervised learning scenario, how could we make the initial guess for the number of custers? One option is The Elbow Method.\n",
    "\n",
    "The basic idea behind cluster partitioning methods, such as k-means clustering, is to define clusters such that the total intra-cluster variation, or total within-cluster sum of square (wcss), is minimized. In the Elbow Method, we plot the WCSS against a set of values for 'k' and the location of a bend (elbow) in the plot is generally considered as an indicator of the appropriate number of clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the optimum number of clusters for k-means classification\n",
    "wcss = [] #within cluster sum of squares\n",
    "\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    kmeans.fit(features_PCA)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    \n",
    "#Plotting the results onto a line graph, allowing us to observe 'The elbow'\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('The elbow method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS') \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What is the optimum number of clusters according to the Elbow Method? \n",
    "\n",
    "Answer: \n",
    "### 7.4 GMM\n",
    "\n",
    "As we saw, Kmeans might not always provide the most optimum output because it deos not have any intrinsic measure of probability or uncertainty of cluster assignments. A major limitation of k-means is that the cluster models must be circular: k-means has no built-in way of accounting for oblong or elliptical clusters.\n",
    "\n",
    "Gaussian mixture models (GMMs) offer an extension to the idea of kmeans and provide a better estimation. They attempt to find a mixture of multi-dimensional Gaussian probability distributions that best model any input dataset. While Kmeans is a method that performs hard labeling, i.e., it simply choses the maximum probability, GMM provide soft labeling by looking at all the probabilities instead of only maximum. \n",
    "**Task:** Try out GMM using `sklearn`'s `GaussianMixture`. Display the probabilities that are assigned to every sample to understand the concept of soft-labeling as explained above. You may also plot the clusters for visualization. \n",
    "\n",
    "Tip: Round the probabilities up to two decimal places before displaying.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback Cell:\n",
    "I hope this Notebook gave you a good start into python and Machine Learning. Let us know how you liked it. Any suggestions/ criticism are also welcome! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Feedback: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgements\n",
    "\n",
    "The iris pictures are licensed with [CC BY-SA 3.0 DEED](https://creativecommons.org/licenses/by-sa/3.0/) from https://w.wiki/9hfQ and https://w.wiki/9hfR, and [CC BY-SA 2.0 DEED](https://creativecommons.org/licenses/by-sa/2.0/deed.en) from https://w.wiki/9hfT. The [iris dataset](https://doi.org/10.24432/C56C76) from Fisher (1936) is licensed under [CC BY 4.0 LEGAL CODE](https://creativecommons.org/licenses/by/4.0/legalcode). The libraries used are antigravity, numpy, matplotlib, seaborn, sklearn, opencv, etc. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
