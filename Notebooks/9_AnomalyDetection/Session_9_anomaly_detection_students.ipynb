{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3357d7bd-3cd7-4740-ada6-d94ed2dc69da",
   "metadata": {},
   "source": [
    "# HaMLeT\n",
    "\n",
    "## Session 9: Zero-shot anomaly segmentation with CLIP\n",
    "by Jin Er, Lehrstuhl für Bildverarbeitung der RWTH Aachen\n",
    "\n",
    "### Goal of this Session\n",
    "\n",
    "In this session, you will step by step, implement a SCLIPAD algorithm yourself. You should already be familiar with CLIP concepts, the \"torch\" library, and familiar with MVTec datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9017dffd-b806-4e14-9d4a-9dbca5bb9bc3",
   "metadata": {},
   "source": [
    "### import all the relevant libraries\n",
    "\n",
    "please run `!pip install` to install the necessary python `libraries`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbf0f48-4811-42dc-ad04-9554eca9f0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238ff978-4f1c-40ea-8bc5-fd5226af990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# base\n",
    "import open_clip\n",
    "import torch\n",
    "import einops\n",
    "from torchvision import transforms\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from text_prompt import (\n",
    "    STATE_LEVEL_NORMAL_PROMPTS,\n",
    "    STATE_LEVEL_ABNORMAL_PROMPTS,\n",
    "    TEMPLATE_LEVEL_PROMPTS\n",
    ")\n",
    "\n",
    "# debugging \n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ef40c3-38bc-4117-9cab-cd6e4a5390db",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "### Task 0: MVTec dataloader\n",
    "\n",
    "The MVTec AD dataset is a comprehensive real-world dataset for benchmarking anomaly detection algorithms, particularly in the field of industrial inspection. Developed by MVTec Software GmbH, a leading provider of industrial machine vision software, this dataset is specifically designed to aid in the development and evaluation of anomaly detection methods.\n",
    "\n",
    "In the MVTec AD dataset, the 15 categories are divided between object and texture types. Each category represents a distinct class of industrial product or material. Here are the specific names of these categories:\n",
    "\n",
    "Object Categories:\n",
    "1. **Bottle**\n",
    "2. **Cable**\n",
    "3. **Capsule**\n",
    "4. **Carpet**\n",
    "5. **Grid**\n",
    "6. **Hazelnut**\n",
    "7. **Leather**\n",
    "8. **Metal Nut**\n",
    "9. **Pill**\n",
    "10. **Screw**\n",
    "11. **Tile**\n",
    "12. **Toothbrush**\n",
    "13. **Transistor**\n",
    "14. **Wood**\n",
    "15. **Zipper**\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"imgs/MVTecAD-0000003433-bf7e8d4c.jpg\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "Each of these categories encompasses various examples of normal conditions as well as a range of anomalies or defects relevant to the specific type of object or material. This categorization helps in addressing diverse challenges in industrial anomaly detection and quality control.\n",
    "\n",
    "**IMPORTANT:** To be able to solve this notebook you need to request access for MVTec AD dataset yourself from here: https://www.mvtec.com/company/research/datasets/mvtec-ad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7167b00e-ad3e-4ea2-b0ce-2f1578e4c765",
   "metadata": {},
   "source": [
    "### Task 1: Loader MVTec dataloader, for the simplicity, we are only use category `wood` during this session, and plot one image out from the `test.dataset`\n",
    "\n",
    "Please read the `dataloader.py` file and try to understand the concept of `lightning dataModule` and `torch.dataset`. \n",
    "\n",
    "**DO NOT MODIFY the dataloader file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5022b7-4eb4-47c7-a90c-7770d9128e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the mvtec dataloader\n",
    "from dataloader import MVTecDataModule\n",
    "\n",
    "# -------------- student solution -----------------\n",
    "# The solution for each part can be done via typing one or max two lines of code, \n",
    "# if not, it is highly likely you make the mistakes or misunderstand the question\n",
    "\n",
    "\"\"\"\n",
    "dataloader_mvtec = \n",
    "test_dataset = \n",
    "\"\"\"\n",
    "# ------------ end of student solution ------------\n",
    "\n",
    "ds = iter(test_dataset)\n",
    "\n",
    "# only select the image contains anomalies \n",
    "while 1:\n",
    "    data = ds.next()\n",
    "    if data[1] != 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a777c0-d4c4-4b0e-bb16-d0db3ec711a9",
   "metadata": {},
   "source": [
    "### Task 2: Plot the testing image with its respective mask \n",
    "\n",
    "data is a `list` containing RGB images, output (0 for normal image, 1 for abnormal image), binary mask\n",
    "\n",
    "The primary goal of this task is plotting the original image with its respective mask. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb64783-43f7-4612-9dce-7a0beb10e0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_input_img(\n",
    "    data: list = data,\n",
    "    pred = None, \n",
    "):\n",
    "    invTrans = transforms.Compose(\n",
    "        [\n",
    "            transforms.Normalize(\n",
    "                mean = [ 0., 0., 0. ],\n",
    "                std = [ 1/0.229, 1/0.224, 1/0.225 ]\n",
    "            ),\n",
    "            transforms.Normalize(\n",
    "                mean = [ -0.485, -0.456, -0.406 ],\n",
    "                std = [ 1., 1., 1. ]\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    def convert_to_numpy(input, inverse=False):\n",
    "        \"help function to convert torch tensor to numpy for plotting images\"\n",
    "        if inverse:\n",
    "            input = invTrans(input)\n",
    "        input = einops.rearrange(input, \"b c w h -> b w h c\")[0]\n",
    "        return input.detach().cpu().numpy()\n",
    "        \n",
    "    # -------------- student solution  -----------------\n",
    "\n",
    "    # convert the image and mask\n",
    "\n",
    "    imgs: list\n",
    "\n",
    "    # --------- end of student solution --------------\n",
    "\n",
    "    if pred is not None:\n",
    "        imgs.append(convert_to_numpy(pred)[0])\n",
    "\n",
    "    num_imgs = len(imgs)\n",
    "    fig, axes = plt.subplots(1, num_imgs, figsize=(10 * num_imgs, 10))\n",
    "    # Plot each image\n",
    "    for ax, i in zip(axes, imgs):\n",
    "        if i.shape[-1] == 1:\n",
    "            ax.imshow(i, cmap=\"gray\")\n",
    "        else:\n",
    "            ax.imshow(i)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()  # Adjust subplots to fit into the figure area.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723636f3-0b27-4ea3-a08a-18130b82470d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_input_img()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c43da81-f623-418b-9d1e-cfdbe1584013",
   "metadata": {},
   "source": [
    "### Task 3: Load CLIP visual encoder and text encoder\n",
    "\n",
    "Please check the `utils.py` \n",
    "\n",
    "CLIP framework contains two networks, image encoder and text encoder; in our session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d66187-730c-45dc-a1dc-a78ba24522a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "\n",
    "from utils import get_clip_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f236a797-e31d-49cc-9a93-0178baa1e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------- student solution  -----------------\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "# use print(model) to print the general structure of the model\n",
    "vit = None\n",
    "# ------------end of student solution-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046d91d8-8814-4cd5-ada0-edfd87625519",
   "metadata": {},
   "source": [
    "### Task 4:\n",
    "\n",
    "Define a new image encoder with modified attention blocks, as shown in the following figure. \n",
    "\n",
    "<div>\n",
    "<img src=\"imgs/mll_lab.jpg\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "Fig (a) shows the modified `qq` and `kk` attention instead of the original` qk `attention from the attention block. \n",
    "Fig (b) shows the general CLIP framework.\n",
    "\n",
    "We need to rewrite the image encoder for applying the modify `qq` `kk` attention. \n",
    "\n",
    "#### Please note that we only apply the modification in the last resblock in ViT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50bbd21-fa6f-43a5-9f39-cd7069425412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_encoder(\n",
    "    x, \n",
    "    vit=vit,\n",
    "    layer_num=11,\n",
    "    normalize=True,\n",
    "):\n",
    "    x = vit.conv1(x)\n",
    "    x = x.reshape(x.shape[0], x.shape[1], -1)\n",
    "    x = x.permute(0, 2, 1)\n",
    "\n",
    "    # class embedding and positional embeddings\n",
    "    x = torch.cat(\n",
    "        [vit.class_embedding.to(x.dtype) + \n",
    "         torch.zeros(\n",
    "             x.shape[0], 1, x.shape[-1], \n",
    "             dtype=x.dtype, \n",
    "             device=x.device\n",
    "         ),\n",
    "         x], dim=1\n",
    "        )  # shape = [*, grid ** 2 + 1, width]\n",
    "    \n",
    "    x = x + vit.positional_embedding.to(x.dtype)\n",
    "    x = vit.patch_dropout(x)\n",
    "    x = vit.ln_pre(x)\n",
    "    x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "\n",
    "    def csa_attn(resblock, x):\n",
    "        attn = resblock.attn\n",
    "        num_heads = attn.num_heads\n",
    "        _, B, E = x.size()\n",
    "        head_dim = E // num_heads\n",
    "        scale = head_dim ** -0.5\n",
    "\n",
    "        qkv = F.linear(x, attn.in_proj_weight, attn.in_proj_bias)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)  # [L, B, E]\n",
    "\n",
    "        # [B * num_head, L (sequence length), E // num_head)\n",
    "        q = q.contiguous().view(-1, B * num_heads, head_dim).transpose(0, 1)\n",
    "        k = k.contiguous().view(-1, B * num_heads, head_dim).transpose(0, 1)\n",
    "        v = v.contiguous().view(-1, B * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "        #------------    student solution     ---------\n",
    "\n",
    "        # please fulfil your solution here, and DO NOT CHANGE THE OTHER PART OF CODE!!!\n",
    "\n",
    "        # part one calculate qq attetion and kk attention weight\n",
    "\n",
    "        # get attention weight\n",
    "        \n",
    "        #------------    end of student solution     ------------\n",
    "\n",
    "        attn_output = attn_weights @ v  # [B * num_head, L, E // num_head]\n",
    "        attn_output = attn_output.transpose(0, 1).contiguous().view(-1, B, E)  # [L, B, E]\n",
    "\n",
    "        attn_output = attn.out_proj(attn_output)\n",
    "        return attn_output\n",
    "\n",
    "    resblocks = vit.transformer.resblocks\n",
    "    for block_idx, block in enumerate(resblocks):\n",
    "        if block_idx >= layer_num:\n",
    "            \"\"\" Applying Self Attention Module in the last attention block \"\"\"\n",
    "            # ----------student solution  ----------\n",
    "            # please check how atten is applied in ViT, if you do not know, please check the following link\n",
    "            # https://github.com/mlfoundations/open_clip/blob/1be2c8993b3f2628d495dfa791061e92f1cd4d0e/src/open_clip/transformer.py#L262\n",
    "\n",
    "            # Hint: the original ResidualAttentionBlock remain the same, please only replace the atten block\n",
    "            # ------------------------------------ \n",
    "            \n",
    "        else:\n",
    "            x = block(x)\n",
    "    x = x.permute(1, 0, 2)\n",
    "\n",
    "    x = vit.ln_post(x)\n",
    "    proj = vit.proj\n",
    "    x = x @ proj\n",
    "\n",
    "    if normalize:\n",
    "        x = F.normalize(x, dim=-1, p=2)\n",
    "\n",
    "    return {\n",
    "        \"cls\": x[:, 0, :],\n",
    "        \"tokens\": x[:, 1:, :]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e239300e-f873-43f8-a71b-7586716975fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DO NOT CHANGE THE CODE HERE\n",
    "# for sanity check \n",
    "\n",
    "encoded_patches = image_encoder(x=data[0])[\"tokens\"]\n",
    "shape = encoded_patches.shape\n",
    "assert shape[1] == 196, shape[2] == 768"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c2972c-b5d3-4502-b481-8b61f4e5a92a",
   "metadata": {},
   "source": [
    "### Task 5: Text Classifier \n",
    "\n",
    "Generate average text embeddings for abnormal text and normal text, please check `text_prompt.py` for the details text prompt architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084624c3-7764-47c1-a8fa-f8ec3c6abc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_encoder(\n",
    "    text,\n",
    "    model = model,\n",
    "    normalize: bool = True\n",
    "):\n",
    "    model = model.eval()\n",
    "    text_features = model.encode_text(text)\n",
    "    if normalize:\n",
    "        text_features = F.normalize(text_features, dim=-1, p=2)\n",
    "    return text_features\n",
    "\n",
    "def build_text_classifier(\n",
    "    model = model,\n",
    "    tokenizer=tokenizer,\n",
    "    category = \"wood\"\n",
    "):\n",
    "    def _process_template(state_level_templates):\n",
    "        text = []\n",
    "        \n",
    "        for template in TEMPLATE_LEVEL_PROMPTS:\n",
    "            for state_template in state_level_templates:\n",
    "                # ------------student version ------------------\n",
    "                #  generate the template normal and abnormal text, please check how to use lambada\n",
    "                #  DO NOT MAKE IT TOO COMPLICATE; MAX 2 Lines of Code is ENOUGH!!!!\n",
    "                # ----------------------------------------------\n",
    "\n",
    "                \n",
    "        device = model.parameters().__next__().device\n",
    "        texts = tokenizer(text).to(device=device)\n",
    "        class_embeddings = text_encoder(texts)\n",
    "        mean_class_embeddings = torch.mean(class_embeddings, dim=0, keepdim=True)\n",
    "        mean_class_embeddings = F.normalize(mean_class_embeddings, dim=-1)\n",
    "        return mean_class_embeddings\n",
    "    normal_text_embedding = _process_template(STATE_LEVEL_NORMAL_PROMPTS)\n",
    "    abnormal_text_embedding = _process_template(STATE_LEVEL_ABNORMAL_PROMPTS)\n",
    "    return torch.cat([normal_text_embedding, abnormal_text_embedding], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690b78ba-bb5a-4cae-8338-abbd8a83bd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = build_text_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fb0f9f-90ab-40f0-84c3-f0636cb4aef0",
   "metadata": {},
   "source": [
    "### Task 6: Generate Anomaly Heatmap\n",
    "\n",
    "Use the text, which is generated in the Task 5 and encoded patches in Task 4 to generate the heatmap\n",
    "\n",
    "Please always normalise text and patch embedding\n",
    "\n",
    "hint: to calculate the cosine similarity !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70974a8-3641-428f-84be-fb49bbe59ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_generation(\n",
    "    img_patches,\n",
    "    text_features,\n",
    "    logit_scale=4.7\n",
    "):\n",
    "    \"\"\"generating patch-wise abnormal score\"\"\"\n",
    "    # ----------- solution -------------------\n",
    "    logit_patches = (img_patches @ text_features.T)\n",
    "    logit_patches = (logit_scale * logit_patches).softmax(dim=-1)\n",
    "    logit_patches = logit_patches.transpose(1, -1)\n",
    "\n",
    "    # Hint: you can change the shape of the image by using einops operation\n",
    "\n",
    "    \n",
    "    #--------------end of solution --------------------------\n",
    "    return logit_patches[:, 1:, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec64ade8-fc08-4184-8784-e27c2b884893",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_patches = heatmap_generation(\n",
    "    img_patches=encoded_patches, \n",
    "    text_features=text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca88da44-d8ab-43ae-8f0f-c30a7a1cf97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_input_img(data, logit_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fd473f-ac47-4239-8ec1-1df42395383d",
   "metadata": {},
   "source": [
    "### Questions \n",
    "\n",
    "What do you think about the prediction mask ? Is there any better way we can improve it ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b33d53b-c187-45cc-92a9-1f4ffbfd87a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd13f17b",
   "metadata": {},
   "source": [
    "**MVTec Dataset Citations**:\n",
    "\n",
    "[1] Paul Bergmann, Kilian Batzner, Michael Fauser, David Sattlegger, Carsten Steger: The MVTec Anomaly Detection Dataset: A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection; in: *International Journal of Computer Vision* 129(4):1038-1059, 2021, DOI: [10.1007/s11263-020-01400-4](https://link.springer.com/article/10.1007/s11263-020-01400-4).\n",
    "\n",
    "[2] Paul Bergmann, Michael Fauser, David Sattlegger, Carsten Steger: MVTec AD — A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection; in: *IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, 9584-9592, 2019, DOI: [10.1109/CVPR.2019.00982](https://ieeexplore.ieee.org/document/8954181)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
