{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HaMLeT\n",
    "\n",
    "## Session 5: Neural Networks\n",
    "by Raphael Kolk\n",
    "\n",
    "### Goal of this Session\n",
    "\n",
    "In this session you will, step by step, implement a neural network yourself without using any deep learning libraries. You should already be familiar with Python as well as NumPy (a package for scientific computing with Python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "**Task 0:** Please execute the following cell which is a workaround for data loading and ignore it until further notice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import urllib\n",
    "class MNIST:\n",
    "    def __init__(self):\n",
    "        # Alternative method to load MNIST, if mldata.org is down\n",
    "        from scipy.io import loadmat\n",
    "        mnist_alternative_url = \"https://github.com/amplab/datascience-sp14/raw/master/lab7/mldata/mnist-original.mat\"\n",
    "        mnist_path = \"./mnist-original.mat\"\n",
    "        response = urllib.request.urlopen(mnist_alternative_url)\n",
    "        with open(mnist_path, \"wb\") as f:\n",
    "            content = response.read()\n",
    "            f.write(content)\n",
    "        mnist_raw = loadmat(mnist_path)\n",
    "        self.data = mnist_raw[\"data\"].T\n",
    "        self.target = mnist_raw[\"label\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1a:** Implement the sigmoid function! Make sure it can handle a vector of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    # ---------- Add code in between these comments ----------\n",
    "    # --------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1b:** For later use in the backpropagation algorithm calculate the first derivative of the sigmoid function and implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(z):\n",
    "    # ---------- Add code in between these comments ----------\n",
    "    # --------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1c:** Plot the implemented sigmoid function and its dericative by executing the next cell to ensure that it works properly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-4, 4, num=100)\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.plot(x, sigmoid_derivative(x))\n",
    "plt.legend((\"sigmoid\", \"derivative\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The centerpiece is the `Network` class which we use to represent a neural network. The following cell starts to implement the network object by defining the `__init__(self, ...)` method that is the constructor of the class and initializes the member variables. The biases and weights in the Network object are all initialized randomly using a Gaussian distribution with mean 0 and standard deviation 1. This random initialization gives our stochastic gradient descent algorithm a place to start from.\n",
    "\n",
    "The list `sizes`, which contains the number of neurons in the respective layers of the network, is given as a parameter.  For example, if the list was `[2, 3, 1]`, then it would be a three-layer network with the first layer containing 2 neurons, the second layer 3 neurons, and the third layer 1 neuron. \n",
    "\n",
    "After initialization the class instance should contain the following four member variables:\n",
    "\n",
    "- **`self.num_layers`** is the total number of layers in the network.\n",
    "\n",
    "- **`self.sizes`** is the same as the parameter `sizes`.\n",
    "\n",
    "- **`self.biases`** is a list of NumPy vectors. The biases for each hidden and output layer are stored in the respective vector of the list. The biases for the network are initialized randomly, using a Gaussian distribution with mean 0 and variance 1. Note that the first layer is assumed to be an input layer, and by convention we won't set any biases for those neurons, since biases are only ever used in computing the outputs from preceding layers.\n",
    "\n",
    "- **`self.weights`** is a list of NumPy matrices, each storing the weights of the connection between layers. So for example `self.weights[1]` is a NumPy matrix storing the weights connecting the second and third layers of neurons. (It's not the first and second layers, since Python's list indexing starts at 0.) Since `self.weights[1]` is rather verbose, let's just denote that matrix $w$. It's a matrix such that $w_{jk}$ is the weight for the connection between the $k$-th neuron in the second layer, and the $j$-th neuron in the third layer. \n",
    "\n",
    "To generate random values you are supposed to use the NumPy function `np.random.randn()`. Look up the NumPy documentation for further information on the usage.\n",
    "\n",
    "**Task 2a:** Implement the code initializing the four member variables of the network!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jdc # import for jupyter magic command \"%%add_to\".\n",
    "import random\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"Initialize network with random weights and biases.\"\"\"\n",
    "        # ---------- Add code in between these comments ----------\n",
    "        # --------------------------------------------------------\n",
    "        \n",
    "    def __str__(self):\n",
    "        \"\"\"String representation of class\"\"\"\n",
    "        return \"Number of layers: \" + str(self.num_layers) + \"\\n\" + \\\n",
    "               \"Sizes of layers: \" + str(self.sizes) + \"\\n\" + \\\n",
    "               \"Biases:\" + \"\\n\" + \"\\n\".join([str(s) for s in self.biases]) + \"\\n\" + \\\n",
    "               \"Weights:\" + \"\\n\" + \"\\n\".join([str(s) for s in self.weights])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2b:** In order to verify your implementation, initialize a new `Network` with an input layer of 3 neurons, two hidden layers with 4 neurons each and an output layer of 2 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Add code in between these comments ----------\n",
    "# --------------------------------------------------------\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above the weight matrix `self.weights[1]` denoted as $w$ stores the weights such that $w_{jk}$ is the weight for the connection between the $k$-th neuron in the second layer and the $j$-th neuron in the third layer\n",
    "This ordering of the $j$ and $k$ indices may seem strange - surely it would make more sense to swap the j and k indices around? The big advantage of using this ordering is that it means the vector of activations of the third layer of neurons is:\n",
    "\n",
    "$$a'=\\sigma(wa+b)$$\n",
    "\n",
    "**Task 3a:** Using the earlier implemented `sigmoid()` function implement the `feed_forward()` method, which applies the equation stated above layer by layer to a given input vector `a`.\n",
    "\n",
    "Using the built-in `zip()` iterator helps you keep your code cleaner and thereby easier to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def feed_forward(self, a):\n",
    "    \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "    # ---------- Add code in between these comments ----------\n",
    "    # --------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3b:** In order to verify your implementation feed the randomly initialized network from task 2b the input vector `[1.2, 3.4, 5.6]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = np.array([1.2, 3.4, 5.6]).reshape((-1, 1))\n",
    "print(net.feed_forward(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are having a look at the actual training and validation dataset, which was introduced in the preparation material. \n",
    "\n",
    "Using scikit-learn we can easily import the MNIST dataset. After loading, images and labels are accessible in two `numpy.ndarrays`.\n",
    "\n",
    "- Each of the 70000 rows in `mnist.data` stores one image (28 x 28 pixel) of a handwritten digit as a vector with a length of 784.\n",
    "\n",
    "- `mnist.target` stores the corresponding labels as an integer number.\n",
    "\n",
    "**Task 4a:** To familiarize yourself with the data, extract and plot ten different images of each digit (0 - 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = MNIST()\n",
    "\n",
    "%matplotlib inline \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i in range(0, 10):\n",
    "    for j in range (0, 10):\n",
    "        # ---------- Add code in between these comments ----------\n",
    "        # --------------------------------------------------------\n",
    "        plt.subplot(10, 10, 10*i+j+1)\n",
    "        plt.imshow(image, cmap='gist_yarg')\n",
    "        fig.axes[10*i+j].set_axis_off()\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to design our network with 10 output neurons and train it in a way that the output activation for the digit j becomes 1.0 at the j-th neuron, we need to vectorize our label.\n",
    "\n",
    "**Task 4b:** Implement the function `vectorize()` which accepts an integer $j$ between 0 and 9 and returns a 10-dimensional unit vector with a 1.0 in the $j$-th position and zeroes elsewhere.  This is used to convert a digit (0...9) into a corresponding desired output from the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(j):\n",
    "    # ---------- Add code in between these comments ----------\n",
    "    # --------------------------------------------------------\n",
    "    \n",
    "print(vectorize(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load and prepare the MNIST dataset to be used in the training of our network. Therefore the following steps need to be taken:\n",
    "\n",
    "1. Load the data as we did in task 4a and convert images and labels into lists of samples. This is already implemented in the function below.\n",
    "\n",
    "2. The images in the dataset are integer-valued with a maximum intensity of 255. Before feeding them to the input neuron we have to ensure that their pixel values are between 0.0 and 1.0.\n",
    "\n",
    "3. The labels in the dataset are integers, so we need to vectorize them using the function implemented above.\n",
    "\n",
    "4. We have to merge the list of images and the list of labels so that we end up with one list of tuples. Each tuple contains one image and one label.\n",
    "\n",
    "5. The list of tuples needs to be split into `training_data` (60000 samples) and `validation_data` (10000 samples).\n",
    "\n",
    "**Task 4c:** Implement steps 2 to 5 in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data():\n",
    "    mnist = MNIST()\n",
    "    images = [i.reshape((784,1)) for i in mnist.data]\n",
    "    labels = [int(l) for l in mnist.target]\n",
    "    \n",
    "    # ---------- Add code in between these comments ----------\n",
    "    # --------------------------------------------------------\n",
    "    \n",
    "    return training_data, validation_data\n",
    "\n",
    "training_data, validation_data = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After each training epoch the performance of the network is evaluated by feeding the validation data into the network and comparing the output to the ground truth label. The neural network's output is assumed to be the index of the neuron with the highest activation in the output layer.\n",
    "\n",
    "**Task 5a:** Implement the class method `evaluate()`, which feeds the network the validation data sample by sample using the `feed_forward()` method and return the number of correct classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def evaluate(self, validation_data):\n",
    "    # ---------- Add code in between these comments ----------\n",
    "    # --------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5b:** Test your implemented code by applying the method to a randomly initialized and untrained network. Since the odds for guessing correctly are 1 to 10 the evaluation output should be roughly 10%. You can execute the code several times to test the assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data = load_data()\n",
    "net = Network([784, 30, 10])\n",
    "print(\"Correct classifications of randomly initialized network: {0} of {1}\".format(net.evaluate(validation_data), len(validation_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train()` method, as its name implies, trains our network. The modification of the the `weights` and `biases` itself is done by the already implemented function `update_mini_batch()`, which again uses the backpropagation algorithm to calculate the gradient.\n",
    "\n",
    "However, the `train()` method implements the stochastic gradient descent, which means that the gradient is calculated for a subset of training samples, a so called mini batch, rather than for the whole training set. Besides having other advantages this is a lot faster and shortens the time needed for training. After the network has been fed the training set, split into mini batches, and the parameters have been updated accordingly, the process is repeated in the next epoch, thereby iteratively tuning the network parameters to a near-optimal state.\n",
    "\n",
    "In our implementation of the `train()` method the following steps need to be taken:\n",
    "- The training and validation data is given formatted as described and impemented in task 4c.\n",
    "- For every epoch (the number of epochs is given by `num_epochs`):\n",
    "    - The order of the training data is randomly shuffled.\n",
    "    - The training dataset is split into mini batches of the size `mini_batch_size`.\n",
    "    - The `update_mini_batch()` method is applied to each mini batch.\n",
    "    - The network's perfomance is evaluated and displayed.\n",
    "    \n",
    "**Task 6a:** Implement the missing code in the `train()` method as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def train(self, training_data, validation_data, num_epochs, mini_batch_size, learning_rate):\n",
    "    for j in range(num_epochs):\n",
    "        # ---------- Add code in between these comments ----------\n",
    "        # --------------------------------------------------------\n",
    "\n",
    "def update_mini_batch(self, mini_batch, learning_rate):\n",
    "    nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "    for x, y in mini_batch:\n",
    "        delta_nabla_b, delta_nabla_w = self.backpropagation(x, y)\n",
    "        nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "        nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "    self.weights = [w-(learning_rate/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "    self.biases = [b-(learning_rate/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "def backpropagation(self, x, y):\n",
    "    nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "    # feedforward\n",
    "    activation = x\n",
    "    activations = [x] # list to store all the activations, layer by layer\n",
    "    zs = [] # list to store all the z vectors, layer by layer\n",
    "    for b, w in zip(self.biases, self.weights):\n",
    "        z = np.dot(w, activation)+b\n",
    "        zs.append(z)\n",
    "        activation = sigmoid(z)\n",
    "        activations.append(activation)\n",
    "    # backward pass\n",
    "    delta = (activations[-1] - y) * sigmoid_derivative(zs[-1])\n",
    "    nabla_b[-1] = delta\n",
    "    nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "    for l in range(2, self.num_layers):\n",
    "        z = zs[-l]\n",
    "        sp = sigmoid_derivative(z)\n",
    "        delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "        nabla_b[-l] = delta\n",
    "        nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "    return (nabla_b, nabla_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using all the implemented functionality above you can finally train a network on the MNIST dataset. Therefore:\n",
    "- Load the training and validation data.\n",
    "- Instantiate a new network with\n",
    "    - 784 neurons in the input layer,\n",
    "    - 30 neurons in the first and only hidden layer and\n",
    "    - 10 neurons in the output layer.\n",
    "- Train the network using the training and validation data\n",
    "    - for 30 epochs,\n",
    "    - with a mini batch size of 10 and\n",
    "    - a learning rate of 3.0.\n",
    "    \n",
    "**Task 6b:** Implement the training process as stated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Add code in between these comments ----------\n",
    "# --------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 7:** Have fun changing the training parameters or the design of the network and observing the resulting classification rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Add code in between these comments ----------\n",
    "# --------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback\n",
    "\n",
    "Aaaaaand we're done üëèüèºüçª\n",
    "\n",
    "If you have any suggestions on how we could improve this session, please let us know in the following cell. What did you particularly like or dislike? Did you miss any contents?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8:** Expand your `train` function by a learning rate decay / scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Add code in between these comments ----------\n",
    "# --------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
